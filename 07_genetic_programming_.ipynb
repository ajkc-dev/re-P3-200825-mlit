{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajkc-dev/re-P3-200825-mlit/blob/master/07_genetic_programming_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYhWPv_Sxmhn"
      },
      "source": [
        "In this example, we will be training a symbolic regression using genetic programming.  We will try to fit our model to a set of points.\n",
        "\n",
        "To do this, we will use the library `gplearn` which we install and load with our other libraries below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBhW9Zp8P13h"
      },
      "source": [
        "!pip install gplearn\n",
        "from gplearn.genetic import SymbolicRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0KCDbr61OjM"
      },
      "source": [
        "Next we will create the function for our data and make our training and test data with the function `fitme`. Given an input `x`, our target function will be a simple cubic function: $\\frac{x^3}{10} + x^2$.\n",
        "\n",
        "We’ll make a training set `X_train` for this function consisting of 50 `uniform` random `xs` between -10 and 10. We run these points through our target `fitme` function to get the corresponding targets, `y_train`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxKjFjGsM4pk"
      },
      "source": [
        "def fitme(x):\n",
        "  return(0.1*x*x*x + x*x)\n",
        "\n",
        "# Training samples\n",
        "X_train = np.random.uniform(-10, 10, (50,1))\n",
        "y_train = [fitme(X) for X in X_train]\n",
        "\n",
        "# Testing samples\n",
        "X_test = np.random.uniform(-10, 10, (50,1))\n",
        "y_test = [fitme(X) for X in X_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiBDHHOp6ad4"
      },
      "source": [
        "We will plot the data below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8Pi7NXvUQTc"
      },
      "source": [
        "plt.scatter(X_train, y_train)\n",
        "plt.title('Target distance')\n",
        "plt.xlabel('angle')\n",
        "plt.ylabel('distance')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-zroV7F6cs0"
      },
      "source": [
        "Let's fit a model to this function using genetic programming. The “gplearn” package, first released in 2015, adopts the approach of the Scikit-Learn library and extends it to genetic programming. We install it and import it.\n",
        "\n",
        "Our genetic-programming-based estimator, `est_gp` is a “symbolic regressor”. That is to say, we’re solving a regression problem by finding a symbolic expression --- a little piece of a program.\n",
        "\n",
        "The program it finds will use only two operators: `add` for add and `mul` for multiply. A program can use as many of these operators as necessary. However, there’s a tradeoff in the search between accurately matching the training data and being parsimonious --- using a small expression.  The parsimony_coefficient tells the genetic programming search how much weight to put on parsimony compared to accuracy. Parsimony coefficient can be any non-negative value, where smaller numbers for parsimony tend to result in much bigger programs created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH3deSG8M4vQ"
      },
      "source": [
        "est_gp = SymbolicRegressor(population_size=10000,parsimony_coefficient=0.1,\n",
        "                           function_set=('add', 'mul'))\n",
        "est_gp.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um9DW3om6prk"
      },
      "source": [
        "We can now plot the performance of the model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2DBso_iXjKB"
      },
      "source": [
        "X_lots = np.reshape(np.sort(np.random.uniform(-10, 10, 250)),(-1,1))\n",
        "\n",
        "y_gp = est_gp.predict(X_lots)\n",
        "\n",
        "plt.scatter(X_test, y_test)\n",
        "plt.plot(X_lots, y_gp)\n",
        "plt.title('Target distance')\n",
        "plt.xlabel('angle')\n",
        "plt.ylabel('distance')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmK768JP6_uT"
      },
      "source": [
        "Finally we print the function learned by the genetic program below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSrdTemIM4yO"
      },
      "source": [
        "print(est_gp._program)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}